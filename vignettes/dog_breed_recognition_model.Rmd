---
title: "Dog Breed Recognition V2 - Model"
subtitle: 
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Dog Breed Recognition V2}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Setup

```{r setup}
# libs
library(dogbreed)
library(torch)
library(torchvision)
library(zeallot)

# config
IMG_TRAIN_FOLDER <- "~/datasets/dogs/train/"
IMG_TEST_FOLDER <- "~/datasets/dogs/recognition/test/"
NUM_WORKERS <- 16
BATCH_SIZE <- 32
NUM_EPOCHS <- 20
DEVICE <- torch::torch_device(if(cuda_is_available()) "cuda" else "cpu")
DEVICE
```

## Data prep/data augmentation

```{r}
train_transform <- function(img) {
  img <- img %>%
    # first convert image to tensor
    torchvision::transform_to_tensor() %>%
    # data augmentation
    torchvision::transform_random_affine(15, shear = c(-10, 10)) %>%
    # data augmentation
    torchvision::transform_random_resized_crop(size = c(224, 224)) %>%
    # data augmentation
    torchvision::transform_color_jitter() %>%
    # data augmentation
    torchvision::transform_random_horizontal_flip() %>%
    # normalize according to what is expected by resnet
    torchvision::transform_normalize(
      mean = c(0.485, 0.456, 0.406),
      std = c(0.229, 0.224, 0.225)
    ) 
  img[..,1:224,1:224]
}

test_transform <- function(img) {
  img <- img %>%
    torchvision::transform_to_tensor() %>%
    (function(x) x$to(device = DEVICE)) %>%
    torchvision::transform_resize(size = c(224, 224)) %>%
    torchvision::transform_normalize(
      mean = c(0.485, 0.456, 0.406), 
      std = c(0.229, 0.224, 0.225)
    )
  img[..,1:224,1:224]
}
```

## Datasets

```{r}
train <- image_folder_dataset(IMG_TRAIN_FOLDER, train_transform, loader = magick_loader)
test <- image_folder_dataset(IMG_TRAIN_FOLDER, test_transform, loader = magick_loader)
set.seed(42)
train_idx <- sample.int(length(train), 14000)
train <- torch::dataset_subset(train, train_idx)
test <- torch::dataset_subset(test, setdiff(1:length(train), train_idx))
```

```{r}
train_dl <- dataloader(
  dataset = train,
  batch_size = BATCH_SIZE,
  shuffle = TRUE,
  num_workers = NUM_WORKERS,
  worker_globals = c("DEVICE")
)
test_dl <- dataloader(
  dataset = test,
  batch_size = BATCH_SIZE,
  shuffle = FALSE,
  num_workers = NUM_WORKERS,
  worker_globals = c("DEVICE")
)
```

## Model

Resnet18 with custom 100-output endpoint. Pretrained weigths are frozen.

```{r}
model <- nn_dog(classes = 100, device = DEVICE)
print(model)
```

## Training loop

```{r}
optimizer <- optim_sgd(model$parameters, lr = 0.05, momentum = 0.98)
scheduler <- lr_one_cycle(
  optimizer, 
  max_lr = 0.05, 
  epochs = NUM_EPOCHS, 
  steps_per_epoch = length(train_dl)
)
loss_function <- nn_cross_entropy_loss()
```

```{r}
num_epochs = NUM_EPOCHS
for(epoch in 1:num_epochs) {
  # progress bar for feedback
  pb <- set_progress_bar(length(train_dl))
  pb$message(glue::glue("Epoch {epoch}/{num_epochs}"))
  
  model$train()
  losses <- rep(NA_real_, 5)
  
  coro::loop(for(b in train_dl) {
    optimizer$zero_grad()
    output <- model(b[[1]]$to(device = DEVICE))
    target <- b[[2]]$to(device = DEVICE)
    loss <- loss_function(output, target)
    loss$backward()
    optimizer$step()
    scheduler$step()
    
    losses <- c(loss$item(), losses)
    pb$tick(tokens = list(loss = round(mean(losses[1:5], na.rm = TRUE), 4)))
  })
  
  model$eval()
}
```

```{r}
torch_save(model, "model")
```

## Evalutaion

```{r}
eval <- data.frame(output = NULL, target = NULL)
coro::loop(for(b in test_dl) {
  output <- torch_argmax(model(b[[1]]$to(device = DEVICE)), 2)
  target <- b[[2]]$to(device = DEVICE)
  
  eval_batch <- data.frame(
    output = names(train$dataset$class_to_idx[as_array(output$to(device = "cpu"))]),
    target = names(train$dataset$class_to_idx[as_array(target$to(device = "cpu"))])  
  )
  
  eval <- rbind(eval, eval_batch)
})

eval %>% 
  dplyr::filter(output == target)
```

